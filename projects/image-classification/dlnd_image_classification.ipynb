{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9351d91278>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    r = x.max() - x.min()\n",
    "    return (x - x.min()) / r\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    n = np.zeros(shape=(len(x), 10))\n",
    "    n[np.arange(len(x)), x] = 1\n",
    "    return n\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, (None, ) + image_shape,\n",
    "                          name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, n_classes],\n",
    "                          name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # set weight and bias\n",
    "    input_shape = x_tensor.shape.as_list()[-1]\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],\n",
    "                                              input_shape, conv_num_outputs],\n",
    "                                             stddev=0.04))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # create conv2d\n",
    "    conv = tf.nn.conv2d(x_tensor, weight,\n",
    "                        strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "                        padding='SAME')\n",
    "    \n",
    "    # add bias\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    \n",
    "    # add non-linear activation function\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    # max pooling\n",
    "    conv = tf.nn.max_pool(conv,\n",
    "                          ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                          strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_size = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor, [-1, np.prod(np.array(x_size[1:]))])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.nn.relu(output(x_tensor, num_outputs))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_shape = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable(tf.truncated_normal([input_shape, num_outputs],\n",
    "                                        stddev=0.04))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, [2, 2], [1, 1], [2, 2], [2, 2])\n",
    "\n",
    "    conv2 = conv2d_maxpool(conv1, 64, [3, 3], [1, 1], [1, 1], [1, 1])\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "    conv3 = conv2d_maxpool(conv2, 128, [2, 2], [1, 1], [2, 2], [2, 2])\n",
    "    conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_conv = flatten(conv3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_1 = fully_conn(flatten_conv, 1024)\n",
    "    fully_1 = tf.nn.dropout(fully_1, keep_prob)\n",
    "    \n",
    "#     fully_2 = fully_conn(fully_1, 512)\n",
    "#     fully_2 = tf.nn.dropout(fully_2, keep_prob)\n",
    "    \n",
    "#     fully_3 = fully_conn(fully_2, 256)\n",
    "#     fully_3 = tf.nn.dropout(fully_3, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logit = output(fully_1, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logit\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch,\n",
    "                                      y: label_batch,\n",
    "                                      keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                        y: label_batch,\n",
    "                                        keep_prob: 1})\n",
    "    \n",
    "    training_accuracy = session.run(accuracy, feed_dict={x: feature_batch,\n",
    "                                                         y: label_batch,\n",
    "                                                         keep_prob: 1})\n",
    "    \n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features,\n",
    "                                                           y: valid_labels,\n",
    "                                                           keep_prob: 1})\n",
    "    \n",
    "    print('Loss: {:>10.6f} Training Accuracy: {:.6f} Validation Accuracy: {:.6f}'.format(loss, training_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:   2.248720 Training Accuracy: 0.212871 Validation Accuracy: 0.176200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:   2.101175 Training Accuracy: 0.256188 Validation Accuracy: 0.269600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:   2.043768 Training Accuracy: 0.285891 Validation Accuracy: 0.280200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:   1.989102 Training Accuracy: 0.297030 Validation Accuracy: 0.313600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:   1.920203 Training Accuracy: 0.332921 Validation Accuracy: 0.341400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:   1.847127 Training Accuracy: 0.382426 Validation Accuracy: 0.373600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:   1.840161 Training Accuracy: 0.347772 Validation Accuracy: 0.352600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:   1.749063 Training Accuracy: 0.397277 Validation Accuracy: 0.383200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:   1.693885 Training Accuracy: 0.424505 Validation Accuracy: 0.404800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:   1.687565 Training Accuracy: 0.435644 Validation Accuracy: 0.404000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:   1.628638 Training Accuracy: 0.454208 Validation Accuracy: 0.424200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:   1.586449 Training Accuracy: 0.457921 Validation Accuracy: 0.434000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:   1.568784 Training Accuracy: 0.474010 Validation Accuracy: 0.439400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:   1.540082 Training Accuracy: 0.482673 Validation Accuracy: 0.443200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:   1.494817 Training Accuracy: 0.493812 Validation Accuracy: 0.452800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:   1.491213 Training Accuracy: 0.501238 Validation Accuracy: 0.455800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:   1.447256 Training Accuracy: 0.519802 Validation Accuracy: 0.463400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:   1.411814 Training Accuracy: 0.525990 Validation Accuracy: 0.468200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:   1.439150 Training Accuracy: 0.519802 Validation Accuracy: 0.464600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:   1.379383 Training Accuracy: 0.535891 Validation Accuracy: 0.479800\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:   1.340505 Training Accuracy: 0.550743 Validation Accuracy: 0.493400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:   1.298238 Training Accuracy: 0.571782 Validation Accuracy: 0.505000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:   1.312552 Training Accuracy: 0.545792 Validation Accuracy: 0.477400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:   1.319164 Training Accuracy: 0.556931 Validation Accuracy: 0.496000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:   1.232700 Training Accuracy: 0.594059 Validation Accuracy: 0.505800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:   1.198123 Training Accuracy: 0.613861 Validation Accuracy: 0.521600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:   1.178704 Training Accuracy: 0.603960 Validation Accuracy: 0.517800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:   1.182156 Training Accuracy: 0.620050 Validation Accuracy: 0.524200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:   1.133952 Training Accuracy: 0.631188 Validation Accuracy: 0.524400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:   1.121639 Training Accuracy: 0.638614 Validation Accuracy: 0.534600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:   1.076704 Training Accuracy: 0.650990 Validation Accuracy: 0.542400\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:   1.040107 Training Accuracy: 0.662129 Validation Accuracy: 0.545400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:   1.016992 Training Accuracy: 0.670792 Validation Accuracy: 0.545600\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:   0.996656 Training Accuracy: 0.683168 Validation Accuracy: 0.548000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:   0.965707 Training Accuracy: 0.696782 Validation Accuracy: 0.550200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:   0.962006 Training Accuracy: 0.689356 Validation Accuracy: 0.542200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:   0.956026 Training Accuracy: 0.694307 Validation Accuracy: 0.548400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:   0.941579 Training Accuracy: 0.707921 Validation Accuracy: 0.546200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:   0.941563 Training Accuracy: 0.686881 Validation Accuracy: 0.543600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:   0.878193 Training Accuracy: 0.726485 Validation Accuracy: 0.554200\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:   0.855070 Training Accuracy: 0.748762 Validation Accuracy: 0.564200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:   0.830442 Training Accuracy: 0.747525 Validation Accuracy: 0.566200\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:   0.799620 Training Accuracy: 0.767327 Validation Accuracy: 0.573600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:   0.758152 Training Accuracy: 0.778465 Validation Accuracy: 0.576400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:   0.748461 Training Accuracy: 0.782178 Validation Accuracy: 0.577000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:   0.735220 Training Accuracy: 0.784653 Validation Accuracy: 0.575200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:   0.704901 Training Accuracy: 0.795792 Validation Accuracy: 0.580600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:   0.678736 Training Accuracy: 0.815594 Validation Accuracy: 0.582000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:   0.656302 Training Accuracy: 0.823020 Validation Accuracy: 0.584800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:   0.628642 Training Accuracy: 0.836634 Validation Accuracy: 0.588200\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:   0.604435 Training Accuracy: 0.842822 Validation Accuracy: 0.589200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:   0.585978 Training Accuracy: 0.852723 Validation Accuracy: 0.594200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:   0.566369 Training Accuracy: 0.872525 Validation Accuracy: 0.591800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:   0.551418 Training Accuracy: 0.868812 Validation Accuracy: 0.593200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:   0.530388 Training Accuracy: 0.875000 Validation Accuracy: 0.596000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:   0.509616 Training Accuracy: 0.886139 Validation Accuracy: 0.596400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:   0.489699 Training Accuracy: 0.896040 Validation Accuracy: 0.596800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:   0.470746 Training Accuracy: 0.905941 Validation Accuracy: 0.599600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:   0.449483 Training Accuracy: 0.915842 Validation Accuracy: 0.602000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:   0.430171 Training Accuracy: 0.920792 Validation Accuracy: 0.604400\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:   0.417029 Training Accuracy: 0.920792 Validation Accuracy: 0.606400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:   0.402370 Training Accuracy: 0.935643 Validation Accuracy: 0.610000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:   0.391402 Training Accuracy: 0.931931 Validation Accuracy: 0.607800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:   0.369004 Training Accuracy: 0.943069 Validation Accuracy: 0.610800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:   0.340391 Training Accuracy: 0.950495 Validation Accuracy: 0.612800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:   0.334801 Training Accuracy: 0.954208 Validation Accuracy: 0.614600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:   0.313237 Training Accuracy: 0.956683 Validation Accuracy: 0.614800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:   0.299180 Training Accuracy: 0.957921 Validation Accuracy: 0.613800\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:   0.288422 Training Accuracy: 0.966584 Validation Accuracy: 0.619800\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:   0.277102 Training Accuracy: 0.967822 Validation Accuracy: 0.612800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:   0.263427 Training Accuracy: 0.967822 Validation Accuracy: 0.621200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:   0.247175 Training Accuracy: 0.970297 Validation Accuracy: 0.618600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:   0.228907 Training Accuracy: 0.975248 Validation Accuracy: 0.623000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:   0.220937 Training Accuracy: 0.976485 Validation Accuracy: 0.618000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:   0.216852 Training Accuracy: 0.983911 Validation Accuracy: 0.622400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:   0.208014 Training Accuracy: 0.981436 Validation Accuracy: 0.619200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:   0.197327 Training Accuracy: 0.981436 Validation Accuracy: 0.627600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:   0.181319 Training Accuracy: 0.987624 Validation Accuracy: 0.623400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:   0.171005 Training Accuracy: 0.987624 Validation Accuracy: 0.619400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:   0.152236 Training Accuracy: 0.990099 Validation Accuracy: 0.628600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:   0.152533 Training Accuracy: 0.993812 Validation Accuracy: 0.624200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:   0.174477 Training Accuracy: 0.988861 Validation Accuracy: 0.613000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:   0.144440 Training Accuracy: 0.987624 Validation Accuracy: 0.628800\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:   0.141127 Training Accuracy: 0.990099 Validation Accuracy: 0.623800\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:   0.139026 Training Accuracy: 0.990099 Validation Accuracy: 0.622400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:   0.117106 Training Accuracy: 0.996287 Validation Accuracy: 0.631800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:   0.113019 Training Accuracy: 0.993812 Validation Accuracy: 0.632800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:   0.111177 Training Accuracy: 0.998762 Validation Accuracy: 0.631400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:   0.099673 Training Accuracy: 0.996287 Validation Accuracy: 0.630000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:   0.093011 Training Accuracy: 0.996287 Validation Accuracy: 0.632800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:   0.089783 Training Accuracy: 0.997525 Validation Accuracy: 0.631600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:   0.086409 Training Accuracy: 0.996287 Validation Accuracy: 0.632600\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:   0.076831 Training Accuracy: 0.998762 Validation Accuracy: 0.632400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:   0.079793 Training Accuracy: 0.998762 Validation Accuracy: 0.629000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:   0.070540 Training Accuracy: 0.998762 Validation Accuracy: 0.633200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:   0.068167 Training Accuracy: 0.998762 Validation Accuracy: 0.633400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:   0.067863 Training Accuracy: 0.998762 Validation Accuracy: 0.631800\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:   0.063266 Training Accuracy: 1.000000 Validation Accuracy: 0.635000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:   0.059078 Training Accuracy: 0.998762 Validation Accuracy: 0.635200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:   0.055541 Training Accuracy: 1.000000 Validation Accuracy: 0.637200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:   2.246719 Training Accuracy: 0.212871 Validation Accuracy: 0.186400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:   2.115129 Training Accuracy: 0.256188 Validation Accuracy: 0.250000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:   2.085753 Training Accuracy: 0.278465 Validation Accuracy: 0.264000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:   1.951391 Training Accuracy: 0.335396 Validation Accuracy: 0.311200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:   1.896516 Training Accuracy: 0.352723 Validation Accuracy: 0.328600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:   1.891296 Training Accuracy: 0.352723 Validation Accuracy: 0.354200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:   1.806659 Training Accuracy: 0.402228 Validation Accuracy: 0.371000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:   1.690190 Training Accuracy: 0.414604 Validation Accuracy: 0.393400\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:   1.680238 Training Accuracy: 0.419554 Validation Accuracy: 0.400400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:   1.662311 Training Accuracy: 0.439356 Validation Accuracy: 0.407000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:   1.684115 Training Accuracy: 0.414604 Validation Accuracy: 0.418600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:   1.615724 Training Accuracy: 0.459158 Validation Accuracy: 0.420600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:   1.537597 Training Accuracy: 0.459158 Validation Accuracy: 0.440000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:   1.540477 Training Accuracy: 0.452970 Validation Accuracy: 0.446000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:   1.527402 Training Accuracy: 0.490099 Validation Accuracy: 0.448200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:   1.553059 Training Accuracy: 0.477723 Validation Accuracy: 0.457800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:   1.503075 Training Accuracy: 0.481436 Validation Accuracy: 0.462000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:   1.450546 Training Accuracy: 0.488861 Validation Accuracy: 0.463600\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:   1.443430 Training Accuracy: 0.501238 Validation Accuracy: 0.473800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:   1.429185 Training Accuracy: 0.511139 Validation Accuracy: 0.488000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:   1.449610 Training Accuracy: 0.500000 Validation Accuracy: 0.487600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:   1.439416 Training Accuracy: 0.503713 Validation Accuracy: 0.484800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:   1.365581 Training Accuracy: 0.508663 Validation Accuracy: 0.496200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:   1.353312 Training Accuracy: 0.533416 Validation Accuracy: 0.509800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:   1.367477 Training Accuracy: 0.544554 Validation Accuracy: 0.507200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:   1.372924 Training Accuracy: 0.533416 Validation Accuracy: 0.512600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:   1.348298 Training Accuracy: 0.543317 Validation Accuracy: 0.520000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:   1.317459 Training Accuracy: 0.535891 Validation Accuracy: 0.516800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:   1.315642 Training Accuracy: 0.540842 Validation Accuracy: 0.523400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:   1.277827 Training Accuracy: 0.591584 Validation Accuracy: 0.540600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:   1.316653 Training Accuracy: 0.523515 Validation Accuracy: 0.517600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:   1.289166 Training Accuracy: 0.574257 Validation Accuracy: 0.527600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:   1.255702 Training Accuracy: 0.553218 Validation Accuracy: 0.537400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:   1.232540 Training Accuracy: 0.574257 Validation Accuracy: 0.547200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:   1.207696 Training Accuracy: 0.611386 Validation Accuracy: 0.553800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:   1.217421 Training Accuracy: 0.573020 Validation Accuracy: 0.552000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:   1.209308 Training Accuracy: 0.611386 Validation Accuracy: 0.554800\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:   1.167875 Training Accuracy: 0.584158 Validation Accuracy: 0.558800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:   1.151862 Training Accuracy: 0.608911 Validation Accuracy: 0.575000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:   1.151359 Training Accuracy: 0.627475 Validation Accuracy: 0.576400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:   1.154321 Training Accuracy: 0.599010 Validation Accuracy: 0.572200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:   1.138565 Training Accuracy: 0.633663 Validation Accuracy: 0.574400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:   1.112142 Training Accuracy: 0.611386 Validation Accuracy: 0.576800\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:   1.094406 Training Accuracy: 0.639851 Validation Accuracy: 0.594400\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:   1.096102 Training Accuracy: 0.637376 Validation Accuracy: 0.587400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:   1.110909 Training Accuracy: 0.615099 Validation Accuracy: 0.591800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:   1.084274 Training Accuracy: 0.642327 Validation Accuracy: 0.595000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:   1.056094 Training Accuracy: 0.625000 Validation Accuracy: 0.594600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:   1.047575 Training Accuracy: 0.655941 Validation Accuracy: 0.602000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:   1.049022 Training Accuracy: 0.649752 Validation Accuracy: 0.598600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:   1.065284 Training Accuracy: 0.638614 Validation Accuracy: 0.605000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:   1.044028 Training Accuracy: 0.660891 Validation Accuracy: 0.605400\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:   1.044131 Training Accuracy: 0.642327 Validation Accuracy: 0.595800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:   1.006742 Training Accuracy: 0.663366 Validation Accuracy: 0.614400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:   1.021149 Training Accuracy: 0.662129 Validation Accuracy: 0.606400\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:   1.036607 Training Accuracy: 0.647277 Validation Accuracy: 0.603600\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:   1.038629 Training Accuracy: 0.657178 Validation Accuracy: 0.600200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:   0.985377 Training Accuracy: 0.663366 Validation Accuracy: 0.610200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:   0.978496 Training Accuracy: 0.664604 Validation Accuracy: 0.614200\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:   0.978711 Training Accuracy: 0.679455 Validation Accuracy: 0.614000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:   0.983275 Training Accuracy: 0.675743 Validation Accuracy: 0.623000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:   0.979467 Training Accuracy: 0.675743 Validation Accuracy: 0.627800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:   0.965904 Training Accuracy: 0.673267 Validation Accuracy: 0.618600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:   0.938263 Training Accuracy: 0.698020 Validation Accuracy: 0.630200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:   0.920968 Training Accuracy: 0.707921 Validation Accuracy: 0.633800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:   0.953505 Training Accuracy: 0.668317 Validation Accuracy: 0.622600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:   0.949664 Training Accuracy: 0.685644 Validation Accuracy: 0.633400\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:   0.912990 Training Accuracy: 0.684406 Validation Accuracy: 0.628000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:   0.898566 Training Accuracy: 0.698020 Validation Accuracy: 0.631400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:   0.887974 Training Accuracy: 0.721535 Validation Accuracy: 0.635600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:   0.914799 Training Accuracy: 0.695545 Validation Accuracy: 0.635000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:   0.920457 Training Accuracy: 0.709158 Validation Accuracy: 0.644800\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:   0.891397 Training Accuracy: 0.707921 Validation Accuracy: 0.634400\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:   0.869796 Training Accuracy: 0.719059 Validation Accuracy: 0.648600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:   0.850709 Training Accuracy: 0.736386 Validation Accuracy: 0.644600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:   0.887946 Training Accuracy: 0.699257 Validation Accuracy: 0.644600\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:   0.880701 Training Accuracy: 0.710396 Validation Accuracy: 0.653000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:   0.852255 Training Accuracy: 0.716584 Validation Accuracy: 0.641200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:   0.826409 Training Accuracy: 0.732673 Validation Accuracy: 0.653600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:   0.815385 Training Accuracy: 0.745049 Validation Accuracy: 0.656400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:   0.861217 Training Accuracy: 0.702970 Validation Accuracy: 0.636200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:   0.885383 Training Accuracy: 0.700495 Validation Accuracy: 0.648600\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:   0.845652 Training Accuracy: 0.722772 Validation Accuracy: 0.643800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:   0.818918 Training Accuracy: 0.732673 Validation Accuracy: 0.645600\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:   0.794345 Training Accuracy: 0.754950 Validation Accuracy: 0.659000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:   0.822609 Training Accuracy: 0.722772 Validation Accuracy: 0.660600\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:   0.827485 Training Accuracy: 0.735148 Validation Accuracy: 0.665800\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:   0.806332 Training Accuracy: 0.753713 Validation Accuracy: 0.653000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:   0.771603 Training Accuracy: 0.752475 Validation Accuracy: 0.658400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:   0.761784 Training Accuracy: 0.766089 Validation Accuracy: 0.662000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:   0.800311 Training Accuracy: 0.737624 Validation Accuracy: 0.659800\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:   0.803279 Training Accuracy: 0.733911 Validation Accuracy: 0.670400\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:   0.755798 Training Accuracy: 0.751238 Validation Accuracy: 0.665800\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:   0.737346 Training Accuracy: 0.761139 Validation Accuracy: 0.665400\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:   0.725171 Training Accuracy: 0.782178 Validation Accuracy: 0.672600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:   0.764219 Training Accuracy: 0.748762 Validation Accuracy: 0.663200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:   0.764687 Training Accuracy: 0.763614 Validation Accuracy: 0.676400\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:   0.725127 Training Accuracy: 0.762376 Validation Accuracy: 0.672200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:   0.702857 Training Accuracy: 0.773515 Validation Accuracy: 0.677000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:   0.700753 Training Accuracy: 0.794554 Validation Accuracy: 0.674000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:   0.729258 Training Accuracy: 0.753713 Validation Accuracy: 0.674000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:   0.740399 Training Accuracy: 0.777228 Validation Accuracy: 0.680800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:   0.720757 Training Accuracy: 0.773515 Validation Accuracy: 0.665800\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:   0.693908 Training Accuracy: 0.777228 Validation Accuracy: 0.683800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:   0.678817 Training Accuracy: 0.801980 Validation Accuracy: 0.680200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:   0.708092 Training Accuracy: 0.768564 Validation Accuracy: 0.677000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:   0.711814 Training Accuracy: 0.785891 Validation Accuracy: 0.685400\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:   0.689040 Training Accuracy: 0.783416 Validation Accuracy: 0.673800\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:   0.676632 Training Accuracy: 0.799505 Validation Accuracy: 0.683200\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:   0.666260 Training Accuracy: 0.809406 Validation Accuracy: 0.677800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:   0.682835 Training Accuracy: 0.778465 Validation Accuracy: 0.683400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:   0.702792 Training Accuracy: 0.787129 Validation Accuracy: 0.686000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:   0.691567 Training Accuracy: 0.789604 Validation Accuracy: 0.671400\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:   0.647657 Training Accuracy: 0.783416 Validation Accuracy: 0.683600\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:   0.631164 Training Accuracy: 0.816832 Validation Accuracy: 0.689000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:   0.660624 Training Accuracy: 0.789604 Validation Accuracy: 0.679400\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:   0.669516 Training Accuracy: 0.803218 Validation Accuracy: 0.691200\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:   0.638446 Training Accuracy: 0.815594 Validation Accuracy: 0.683600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:   0.618627 Training Accuracy: 0.814356 Validation Accuracy: 0.691600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:   0.607071 Training Accuracy: 0.827970 Validation Accuracy: 0.690200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:   0.627512 Training Accuracy: 0.801980 Validation Accuracy: 0.689800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:   0.638142 Training Accuracy: 0.814356 Validation Accuracy: 0.701600\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:   0.621873 Training Accuracy: 0.815594 Validation Accuracy: 0.690400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:   0.604873 Training Accuracy: 0.813119 Validation Accuracy: 0.690600\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:   0.590186 Training Accuracy: 0.834158 Validation Accuracy: 0.695000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:   0.616597 Training Accuracy: 0.811881 Validation Accuracy: 0.689800\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:   0.612413 Training Accuracy: 0.818069 Validation Accuracy: 0.704600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:   0.582151 Training Accuracy: 0.840347 Validation Accuracy: 0.702200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:   0.575574 Training Accuracy: 0.823020 Validation Accuracy: 0.690600\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:   0.556713 Training Accuracy: 0.842822 Validation Accuracy: 0.700600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:   0.578689 Training Accuracy: 0.823020 Validation Accuracy: 0.694600\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:   0.593748 Training Accuracy: 0.829208 Validation Accuracy: 0.705400\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:   0.561468 Training Accuracy: 0.850248 Validation Accuracy: 0.702200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:   0.544565 Training Accuracy: 0.836634 Validation Accuracy: 0.695400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:   0.526204 Training Accuracy: 0.860148 Validation Accuracy: 0.708800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:   0.551048 Training Accuracy: 0.837871 Validation Accuracy: 0.704200\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:   0.563913 Training Accuracy: 0.835396 Validation Accuracy: 0.706800\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:   0.544627 Training Accuracy: 0.855198 Validation Accuracy: 0.701600\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:   0.518929 Training Accuracy: 0.855198 Validation Accuracy: 0.701400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:   0.505964 Training Accuracy: 0.862624 Validation Accuracy: 0.710000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:   0.541672 Training Accuracy: 0.834158 Validation Accuracy: 0.700800\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:   0.538307 Training Accuracy: 0.852723 Validation Accuracy: 0.704400\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:   0.513382 Training Accuracy: 0.862624 Validation Accuracy: 0.711200\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:   0.493559 Training Accuracy: 0.863861 Validation Accuracy: 0.710800\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:   0.480108 Training Accuracy: 0.879950 Validation Accuracy: 0.706600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:   0.502439 Training Accuracy: 0.858911 Validation Accuracy: 0.710600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:   0.521931 Training Accuracy: 0.850248 Validation Accuracy: 0.706600\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:   0.497980 Training Accuracy: 0.871287 Validation Accuracy: 0.711400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:   0.470016 Training Accuracy: 0.877475 Validation Accuracy: 0.716800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:   0.476939 Training Accuracy: 0.877475 Validation Accuracy: 0.701600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:   0.498309 Training Accuracy: 0.860148 Validation Accuracy: 0.709200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:   0.496320 Training Accuracy: 0.866337 Validation Accuracy: 0.711400\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:   0.471178 Training Accuracy: 0.883663 Validation Accuracy: 0.710200\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:   0.442390 Training Accuracy: 0.887376 Validation Accuracy: 0.721600\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:   0.434953 Training Accuracy: 0.897277 Validation Accuracy: 0.720800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:   0.460277 Training Accuracy: 0.879951 Validation Accuracy: 0.715000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:   0.466456 Training Accuracy: 0.867574 Validation Accuracy: 0.717600\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:   0.466738 Training Accuracy: 0.878713 Validation Accuracy: 0.708800\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:   0.430250 Training Accuracy: 0.898515 Validation Accuracy: 0.715800\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:   0.417007 Training Accuracy: 0.894802 Validation Accuracy: 0.712800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:   0.455908 Training Accuracy: 0.879951 Validation Accuracy: 0.705400\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:   0.456229 Training Accuracy: 0.875000 Validation Accuracy: 0.715400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:   0.430560 Training Accuracy: 0.894802 Validation Accuracy: 0.721800\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:   0.432687 Training Accuracy: 0.894802 Validation Accuracy: 0.708000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:   0.419677 Training Accuracy: 0.902228 Validation Accuracy: 0.708600\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:   0.422756 Training Accuracy: 0.899752 Validation Accuracy: 0.712000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:   0.423854 Training Accuracy: 0.886139 Validation Accuracy: 0.720000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:   0.414475 Training Accuracy: 0.887376 Validation Accuracy: 0.710000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:   0.400333 Training Accuracy: 0.912129 Validation Accuracy: 0.720400\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:   0.393084 Training Accuracy: 0.920792 Validation Accuracy: 0.711200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:   0.417779 Training Accuracy: 0.898515 Validation Accuracy: 0.708400\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:   0.406675 Training Accuracy: 0.903465 Validation Accuracy: 0.718600\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:   0.400817 Training Accuracy: 0.902228 Validation Accuracy: 0.720200\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:   0.386552 Training Accuracy: 0.913366 Validation Accuracy: 0.720600\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:   0.378847 Training Accuracy: 0.925743 Validation Accuracy: 0.719200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:   0.382851 Training Accuracy: 0.909653 Validation Accuracy: 0.718000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:   0.385624 Training Accuracy: 0.896040 Validation Accuracy: 0.724200\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:   0.368788 Training Accuracy: 0.909653 Validation Accuracy: 0.723200\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:   0.363299 Training Accuracy: 0.934406 Validation Accuracy: 0.717600\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:   0.352143 Training Accuracy: 0.923267 Validation Accuracy: 0.723600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:   0.352967 Training Accuracy: 0.913366 Validation Accuracy: 0.727400\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:   0.368586 Training Accuracy: 0.913366 Validation Accuracy: 0.726800\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:   0.382621 Training Accuracy: 0.905941 Validation Accuracy: 0.716800\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:   0.345337 Training Accuracy: 0.941832 Validation Accuracy: 0.719200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:   0.332872 Training Accuracy: 0.935643 Validation Accuracy: 0.723200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:   0.340119 Training Accuracy: 0.933168 Validation Accuracy: 0.723600\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:   0.358390 Training Accuracy: 0.915842 Validation Accuracy: 0.726000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:   0.355714 Training Accuracy: 0.917079 Validation Accuracy: 0.720600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:   0.326125 Training Accuracy: 0.948020 Validation Accuracy: 0.725400\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:   0.318816 Training Accuracy: 0.936881 Validation Accuracy: 0.722800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:   0.317095 Training Accuracy: 0.935644 Validation Accuracy: 0.726600\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:   0.321788 Training Accuracy: 0.929455 Validation Accuracy: 0.730000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:   0.325179 Training Accuracy: 0.935643 Validation Accuracy: 0.727200\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:   0.305456 Training Accuracy: 0.939356 Validation Accuracy: 0.732400\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:   0.317587 Training Accuracy: 0.940594 Validation Accuracy: 0.718200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:   0.310793 Training Accuracy: 0.944307 Validation Accuracy: 0.727800\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:   0.323833 Training Accuracy: 0.931931 Validation Accuracy: 0.725800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:   0.329040 Training Accuracy: 0.915842 Validation Accuracy: 0.711800\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:   0.291785 Training Accuracy: 0.949257 Validation Accuracy: 0.728600\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:   0.292385 Training Accuracy: 0.945544 Validation Accuracy: 0.719800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:   0.304763 Training Accuracy: 0.948020 Validation Accuracy: 0.726200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:   0.305546 Training Accuracy: 0.946782 Validation Accuracy: 0.729400\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:   0.287900 Training Accuracy: 0.931931 Validation Accuracy: 0.727200\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:   0.299371 Training Accuracy: 0.941832 Validation Accuracy: 0.726800\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:   0.276030 Training Accuracy: 0.949257 Validation Accuracy: 0.727800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:   0.275488 Training Accuracy: 0.950495 Validation Accuracy: 0.735200\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:   0.278058 Training Accuracy: 0.949257 Validation Accuracy: 0.734800\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:   0.260452 Training Accuracy: 0.946782 Validation Accuracy: 0.734000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:   0.255003 Training Accuracy: 0.955446 Validation Accuracy: 0.740200\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:   0.250714 Training Accuracy: 0.962871 Validation Accuracy: 0.735600\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:   0.274645 Training Accuracy: 0.950495 Validation Accuracy: 0.729600\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:   0.267323 Training Accuracy: 0.952970 Validation Accuracy: 0.729200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:   0.249932 Training Accuracy: 0.951733 Validation Accuracy: 0.731600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:   0.252936 Training Accuracy: 0.952970 Validation Accuracy: 0.735000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:   0.241008 Training Accuracy: 0.959158 Validation Accuracy: 0.731000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:   0.245581 Training Accuracy: 0.945544 Validation Accuracy: 0.737000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:   0.248598 Training Accuracy: 0.952970 Validation Accuracy: 0.733600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:   0.240862 Training Accuracy: 0.952970 Validation Accuracy: 0.733800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:   0.237688 Training Accuracy: 0.965347 Validation Accuracy: 0.733400\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:   0.218901 Training Accuracy: 0.970297 Validation Accuracy: 0.739000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:   0.224903 Training Accuracy: 0.957921 Validation Accuracy: 0.738000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:   0.224566 Training Accuracy: 0.967822 Validation Accuracy: 0.737600\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:   0.216454 Training Accuracy: 0.956683 Validation Accuracy: 0.737600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:   0.215529 Training Accuracy: 0.971535 Validation Accuracy: 0.738800\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:   0.215417 Training Accuracy: 0.969059 Validation Accuracy: 0.736600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:   0.218296 Training Accuracy: 0.961634 Validation Accuracy: 0.743600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:   0.215081 Training Accuracy: 0.970297 Validation Accuracy: 0.745000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:   0.199566 Training Accuracy: 0.962871 Validation Accuracy: 0.746600\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:   0.208654 Training Accuracy: 0.971535 Validation Accuracy: 0.736400\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:   0.196394 Training Accuracy: 0.972772 Validation Accuracy: 0.742200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:   0.199706 Training Accuracy: 0.970297 Validation Accuracy: 0.739600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:   0.195767 Training Accuracy: 0.975248 Validation Accuracy: 0.743000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:   0.191129 Training Accuracy: 0.967822 Validation Accuracy: 0.741400\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:   0.187112 Training Accuracy: 0.981436 Validation Accuracy: 0.745600\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:   0.194143 Training Accuracy: 0.970297 Validation Accuracy: 0.739200\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:   0.214841 Training Accuracy: 0.967822 Validation Accuracy: 0.739000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:   0.204389 Training Accuracy: 0.971535 Validation Accuracy: 0.736600\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:   0.205348 Training Accuracy: 0.956683 Validation Accuracy: 0.733000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:   0.190255 Training Accuracy: 0.977723 Validation Accuracy: 0.742400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:   0.176814 Training Accuracy: 0.985149 Validation Accuracy: 0.740000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:   0.197714 Training Accuracy: 0.972772 Validation Accuracy: 0.738400\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:   0.198551 Training Accuracy: 0.975248 Validation Accuracy: 0.734000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:   0.180616 Training Accuracy: 0.967822 Validation Accuracy: 0.740400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:   0.174141 Training Accuracy: 0.980198 Validation Accuracy: 0.743600\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:   0.165043 Training Accuracy: 0.983911 Validation Accuracy: 0.734800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:   0.173627 Training Accuracy: 0.983911 Validation Accuracy: 0.742000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:   0.173399 Training Accuracy: 0.980198 Validation Accuracy: 0.738200\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:   0.193160 Training Accuracy: 0.971535 Validation Accuracy: 0.728000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:   0.167311 Training Accuracy: 0.983911 Validation Accuracy: 0.739400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:   0.150603 Training Accuracy: 0.985148 Validation Accuracy: 0.736600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:   0.168615 Training Accuracy: 0.975248 Validation Accuracy: 0.739400\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:   0.161446 Training Accuracy: 0.983911 Validation Accuracy: 0.742600\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:   0.158072 Training Accuracy: 0.977723 Validation Accuracy: 0.741800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:   0.148671 Training Accuracy: 0.990099 Validation Accuracy: 0.750000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:   0.140958 Training Accuracy: 0.991337 Validation Accuracy: 0.740400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:   0.158766 Training Accuracy: 0.983911 Validation Accuracy: 0.745600\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:   0.154366 Training Accuracy: 0.983911 Validation Accuracy: 0.742400\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:   0.154394 Training Accuracy: 0.982673 Validation Accuracy: 0.741400\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:   0.147057 Training Accuracy: 0.995049 Validation Accuracy: 0.744200\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:   0.144219 Training Accuracy: 0.988861 Validation Accuracy: 0.737600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:   0.148406 Training Accuracy: 0.987624 Validation Accuracy: 0.742000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:   0.135551 Training Accuracy: 0.988861 Validation Accuracy: 0.748200\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:   0.142705 Training Accuracy: 0.980198 Validation Accuracy: 0.742800\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:   0.139466 Training Accuracy: 0.995049 Validation Accuracy: 0.744200\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:   0.161738 Training Accuracy: 0.985148 Validation Accuracy: 0.726200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:   0.144718 Training Accuracy: 0.985148 Validation Accuracy: 0.745600\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:   0.175315 Training Accuracy: 0.974010 Validation Accuracy: 0.730200\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:   0.158618 Training Accuracy: 0.991337 Validation Accuracy: 0.743400\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:   0.155108 Training Accuracy: 0.993812 Validation Accuracy: 0.736200\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:   0.155505 Training Accuracy: 0.985148 Validation Accuracy: 0.734200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:   0.156934 Training Accuracy: 0.987624 Validation Accuracy: 0.735600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:   0.132604 Training Accuracy: 0.990099 Validation Accuracy: 0.741200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:   0.132333 Training Accuracy: 0.988861 Validation Accuracy: 0.744800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:   0.139858 Training Accuracy: 0.996287 Validation Accuracy: 0.733600\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:   0.126852 Training Accuracy: 0.993812 Validation Accuracy: 0.738800\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:   0.157309 Training Accuracy: 0.982673 Validation Accuracy: 0.730000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:   0.130238 Training Accuracy: 0.991337 Validation Accuracy: 0.748200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:   0.131955 Training Accuracy: 0.991337 Validation Accuracy: 0.740200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:   0.135991 Training Accuracy: 0.993812 Validation Accuracy: 0.735600\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:   0.124817 Training Accuracy: 0.990099 Validation Accuracy: 0.739400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:   0.131469 Training Accuracy: 0.987624 Validation Accuracy: 0.740400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:   0.112723 Training Accuracy: 0.993812 Validation Accuracy: 0.749000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:   0.112023 Training Accuracy: 0.990099 Validation Accuracy: 0.748200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:   0.107369 Training Accuracy: 0.998762 Validation Accuracy: 0.745400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:   0.103521 Training Accuracy: 0.995049 Validation Accuracy: 0.745400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:   0.116587 Training Accuracy: 0.991337 Validation Accuracy: 0.742600\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:   0.108648 Training Accuracy: 0.993812 Validation Accuracy: 0.749000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:   0.109817 Training Accuracy: 0.993812 Validation Accuracy: 0.749200\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:   0.116012 Training Accuracy: 0.997525 Validation Accuracy: 0.733200\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:   0.102614 Training Accuracy: 0.995049 Validation Accuracy: 0.748200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:   0.104821 Training Accuracy: 0.993812 Validation Accuracy: 0.745200\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:   0.102908 Training Accuracy: 0.993812 Validation Accuracy: 0.753200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:   0.093405 Training Accuracy: 0.992574 Validation Accuracy: 0.756000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:   0.092824 Training Accuracy: 0.997525 Validation Accuracy: 0.753400\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:   0.096139 Training Accuracy: 0.992574 Validation Accuracy: 0.749600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:   0.100192 Training Accuracy: 0.993812 Validation Accuracy: 0.746000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:   0.085446 Training Accuracy: 0.993812 Validation Accuracy: 0.749400\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:   0.086443 Training Accuracy: 0.997525 Validation Accuracy: 0.753800\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:   0.092435 Training Accuracy: 1.000000 Validation Accuracy: 0.749000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:   0.089857 Training Accuracy: 0.992574 Validation Accuracy: 0.748800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:   0.093819 Training Accuracy: 0.995049 Validation Accuracy: 0.747200\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:   0.085016 Training Accuracy: 0.995049 Validation Accuracy: 0.755600\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:   0.078667 Training Accuracy: 0.996287 Validation Accuracy: 0.750400\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:   0.081192 Training Accuracy: 0.998762 Validation Accuracy: 0.749200\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:   0.086455 Training Accuracy: 0.992574 Validation Accuracy: 0.752400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:   0.081578 Training Accuracy: 0.996287 Validation Accuracy: 0.753400\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:   0.076714 Training Accuracy: 0.997525 Validation Accuracy: 0.755000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:   0.076424 Training Accuracy: 0.996287 Validation Accuracy: 0.750200\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:   0.081034 Training Accuracy: 0.996287 Validation Accuracy: 0.749000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:   0.082133 Training Accuracy: 0.993812 Validation Accuracy: 0.755400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:   0.087193 Training Accuracy: 0.992574 Validation Accuracy: 0.750200\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:   0.075677 Training Accuracy: 0.996287 Validation Accuracy: 0.754000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:   0.072112 Training Accuracy: 0.997525 Validation Accuracy: 0.751600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:   0.072785 Training Accuracy: 0.998762 Validation Accuracy: 0.751200\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:   0.075987 Training Accuracy: 0.993812 Validation Accuracy: 0.747600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:   0.080716 Training Accuracy: 0.995049 Validation Accuracy: 0.745800\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:   0.071181 Training Accuracy: 0.997525 Validation Accuracy: 0.753000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:   0.065236 Training Accuracy: 1.000000 Validation Accuracy: 0.752000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:   0.066894 Training Accuracy: 0.998762 Validation Accuracy: 0.753600\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:   0.064270 Training Accuracy: 0.996287 Validation Accuracy: 0.755400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:   0.067814 Training Accuracy: 0.996287 Validation Accuracy: 0.756600\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:   0.063194 Training Accuracy: 0.993812 Validation Accuracy: 0.759400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:   0.059652 Training Accuracy: 0.996287 Validation Accuracy: 0.758200\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:   0.059878 Training Accuracy: 1.000000 Validation Accuracy: 0.756000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:   0.059172 Training Accuracy: 0.998762 Validation Accuracy: 0.758200\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:   0.072003 Training Accuracy: 0.993812 Validation Accuracy: 0.752600\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:   0.057802 Training Accuracy: 1.000000 Validation Accuracy: 0.761800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:   0.055661 Training Accuracy: 0.998762 Validation Accuracy: 0.754400\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:   0.061987 Training Accuracy: 1.000000 Validation Accuracy: 0.749800\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:   0.068114 Training Accuracy: 0.998762 Validation Accuracy: 0.747600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:   0.064762 Training Accuracy: 0.998762 Validation Accuracy: 0.753800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:   0.058314 Training Accuracy: 0.997525 Validation Accuracy: 0.758400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:   0.059793 Training Accuracy: 0.998762 Validation Accuracy: 0.753400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:   0.061327 Training Accuracy: 0.998762 Validation Accuracy: 0.751600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:   0.065128 Training Accuracy: 0.993812 Validation Accuracy: 0.743800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:   0.061844 Training Accuracy: 0.995049 Validation Accuracy: 0.755600\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:   0.057545 Training Accuracy: 0.997525 Validation Accuracy: 0.758400\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:   0.056359 Training Accuracy: 1.000000 Validation Accuracy: 0.750600\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:   0.059262 Training Accuracy: 0.997525 Validation Accuracy: 0.747600\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:   0.061327 Training Accuracy: 0.996287 Validation Accuracy: 0.753600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:   0.051621 Training Accuracy: 0.997525 Validation Accuracy: 0.758000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:   0.060322 Training Accuracy: 0.997525 Validation Accuracy: 0.747200\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:   0.049364 Training Accuracy: 0.998762 Validation Accuracy: 0.756600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:   0.063184 Training Accuracy: 1.000000 Validation Accuracy: 0.748000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:   0.059198 Training Accuracy: 0.997525 Validation Accuracy: 0.753000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:   0.059250 Training Accuracy: 0.997525 Validation Accuracy: 0.754000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:   0.057722 Training Accuracy: 0.998762 Validation Accuracy: 0.756600\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:   0.058276 Training Accuracy: 0.997525 Validation Accuracy: 0.754600\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:   0.052638 Training Accuracy: 1.000000 Validation Accuracy: 0.749400\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:   0.061109 Training Accuracy: 0.995049 Validation Accuracy: 0.749400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:   0.047340 Training Accuracy: 1.000000 Validation Accuracy: 0.758000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:   0.044419 Training Accuracy: 1.000000 Validation Accuracy: 0.757000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:   0.042637 Training Accuracy: 1.000000 Validation Accuracy: 0.763000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:   0.045939 Training Accuracy: 1.000000 Validation Accuracy: 0.751800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:   0.050203 Training Accuracy: 0.997525 Validation Accuracy: 0.754800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:   0.040216 Training Accuracy: 1.000000 Validation Accuracy: 0.758000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:   0.054782 Training Accuracy: 0.998762 Validation Accuracy: 0.743200\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:   0.040686 Training Accuracy: 1.000000 Validation Accuracy: 0.759400\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:   0.061530 Training Accuracy: 0.998762 Validation Accuracy: 0.740600\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:   0.046942 Training Accuracy: 0.997525 Validation Accuracy: 0.761400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:   0.048288 Training Accuracy: 1.000000 Validation Accuracy: 0.749000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:   0.044088 Training Accuracy: 1.000000 Validation Accuracy: 0.758200\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:   0.057739 Training Accuracy: 0.996287 Validation Accuracy: 0.746000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:   0.051200 Training Accuracy: 1.000000 Validation Accuracy: 0.746200\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:   0.058474 Training Accuracy: 0.998762 Validation Accuracy: 0.748200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:   0.042206 Training Accuracy: 1.000000 Validation Accuracy: 0.762400\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:   0.060432 Training Accuracy: 0.998762 Validation Accuracy: 0.743800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:   0.038948 Training Accuracy: 0.998762 Validation Accuracy: 0.760800\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:   0.039270 Training Accuracy: 1.000000 Validation Accuracy: 0.754800\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:   0.040596 Training Accuracy: 0.998762 Validation Accuracy: 0.755400\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:   0.033138 Training Accuracy: 1.000000 Validation Accuracy: 0.758800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:   0.036149 Training Accuracy: 1.000000 Validation Accuracy: 0.754800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:   0.032523 Training Accuracy: 1.000000 Validation Accuracy: 0.763000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:   0.034765 Training Accuracy: 1.000000 Validation Accuracy: 0.758200\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:   0.038944 Training Accuracy: 0.997525 Validation Accuracy: 0.755400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:   0.035723 Training Accuracy: 1.000000 Validation Accuracy: 0.761200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:   0.036991 Training Accuracy: 0.998762 Validation Accuracy: 0.757400\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:   0.035849 Training Accuracy: 0.997525 Validation Accuracy: 0.758800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:   0.038716 Training Accuracy: 1.000000 Validation Accuracy: 0.750400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:   0.036271 Training Accuracy: 0.998762 Validation Accuracy: 0.754400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:   0.030628 Training Accuracy: 1.000000 Validation Accuracy: 0.758800\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:   0.039563 Training Accuracy: 0.998762 Validation Accuracy: 0.755400\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:   0.036669 Training Accuracy: 0.998762 Validation Accuracy: 0.757000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:   0.038988 Training Accuracy: 1.000000 Validation Accuracy: 0.751000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:   0.031736 Training Accuracy: 1.000000 Validation Accuracy: 0.754800\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:   0.034097 Training Accuracy: 1.000000 Validation Accuracy: 0.754600\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:   0.034343 Training Accuracy: 1.000000 Validation Accuracy: 0.758800\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:   0.040704 Training Accuracy: 1.000000 Validation Accuracy: 0.748600\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:   0.032736 Training Accuracy: 1.000000 Validation Accuracy: 0.755200\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:   0.040046 Training Accuracy: 0.997525 Validation Accuracy: 0.746800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:   0.033113 Training Accuracy: 1.000000 Validation Accuracy: 0.760600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:   0.034478 Training Accuracy: 1.000000 Validation Accuracy: 0.759600\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:   0.034079 Training Accuracy: 1.000000 Validation Accuracy: 0.758200\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:   0.029249 Training Accuracy: 1.000000 Validation Accuracy: 0.762600\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:   0.026699 Training Accuracy: 1.000000 Validation Accuracy: 0.762600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:   0.025479 Training Accuracy: 1.000000 Validation Accuracy: 0.767200\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:   0.026036 Training Accuracy: 1.000000 Validation Accuracy: 0.767400\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:   0.031981 Training Accuracy: 0.998762 Validation Accuracy: 0.755000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:   0.026688 Training Accuracy: 1.000000 Validation Accuracy: 0.761800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:   0.028197 Training Accuracy: 1.000000 Validation Accuracy: 0.758800\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:   0.025681 Training Accuracy: 1.000000 Validation Accuracy: 0.762000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:   0.032261 Training Accuracy: 1.000000 Validation Accuracy: 0.755400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:   0.035700 Training Accuracy: 1.000000 Validation Accuracy: 0.743600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:   0.027877 Training Accuracy: 1.000000 Validation Accuracy: 0.763400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:   0.028220 Training Accuracy: 1.000000 Validation Accuracy: 0.755000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:   0.027189 Training Accuracy: 1.000000 Validation Accuracy: 0.763200\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:   0.027282 Training Accuracy: 1.000000 Validation Accuracy: 0.757600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:   0.025403 Training Accuracy: 0.998762 Validation Accuracy: 0.765800\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:   0.023355 Training Accuracy: 1.000000 Validation Accuracy: 0.764600\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:   0.022732 Training Accuracy: 1.000000 Validation Accuracy: 0.764600\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:   0.023297 Training Accuracy: 1.000000 Validation Accuracy: 0.760800\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:   0.025392 Training Accuracy: 1.000000 Validation Accuracy: 0.759600\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:   0.024535 Training Accuracy: 0.998762 Validation Accuracy: 0.762800\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:   0.024266 Training Accuracy: 1.000000 Validation Accuracy: 0.758000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:   0.023702 Training Accuracy: 1.000000 Validation Accuracy: 0.764200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:   0.023289 Training Accuracy: 1.000000 Validation Accuracy: 0.760000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:   0.023528 Training Accuracy: 1.000000 Validation Accuracy: 0.758600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:   0.025376 Training Accuracy: 0.998762 Validation Accuracy: 0.761600\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:   0.021761 Training Accuracy: 1.000000 Validation Accuracy: 0.767200\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:   0.021539 Training Accuracy: 1.000000 Validation Accuracy: 0.762000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:   0.021816 Training Accuracy: 1.000000 Validation Accuracy: 0.761000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:   0.022811 Training Accuracy: 1.000000 Validation Accuracy: 0.760600\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:   0.020310 Training Accuracy: 1.000000 Validation Accuracy: 0.761200\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:   0.022524 Training Accuracy: 1.000000 Validation Accuracy: 0.759400\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:   0.018908 Training Accuracy: 1.000000 Validation Accuracy: 0.761400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:   0.019562 Training Accuracy: 1.000000 Validation Accuracy: 0.763400\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:   0.019169 Training Accuracy: 1.000000 Validation Accuracy: 0.763000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:   0.021959 Training Accuracy: 0.998762 Validation Accuracy: 0.762400\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:   0.020854 Training Accuracy: 1.000000 Validation Accuracy: 0.761200\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:   0.017834 Training Accuracy: 1.000000 Validation Accuracy: 0.759400\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:   0.021615 Training Accuracy: 1.000000 Validation Accuracy: 0.761600\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:   0.018859 Training Accuracy: 1.000000 Validation Accuracy: 0.763000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:   0.020592 Training Accuracy: 1.000000 Validation Accuracy: 0.760800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:   0.019017 Training Accuracy: 1.000000 Validation Accuracy: 0.768000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:   0.016397 Training Accuracy: 1.000000 Validation Accuracy: 0.765400\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:   0.018216 Training Accuracy: 1.000000 Validation Accuracy: 0.765800\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:   0.017187 Training Accuracy: 1.000000 Validation Accuracy: 0.768200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:   0.018065 Training Accuracy: 1.000000 Validation Accuracy: 0.761800\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:   0.018888 Training Accuracy: 1.000000 Validation Accuracy: 0.761800\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:   0.016563 Training Accuracy: 1.000000 Validation Accuracy: 0.765600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:   0.017733 Training Accuracy: 1.000000 Validation Accuracy: 0.760800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:   0.016638 Training Accuracy: 1.000000 Validation Accuracy: 0.763800\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:   0.016972 Training Accuracy: 1.000000 Validation Accuracy: 0.767800\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:   0.016105 Training Accuracy: 1.000000 Validation Accuracy: 0.770600\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:   0.012881 Training Accuracy: 1.000000 Validation Accuracy: 0.766400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:   0.014400 Training Accuracy: 1.000000 Validation Accuracy: 0.766400\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:   0.016112 Training Accuracy: 1.000000 Validation Accuracy: 0.761400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:   0.015895 Training Accuracy: 1.000000 Validation Accuracy: 0.768600\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:   0.014678 Training Accuracy: 1.000000 Validation Accuracy: 0.767600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:   0.014945 Training Accuracy: 1.000000 Validation Accuracy: 0.761200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:   0.017386 Training Accuracy: 1.000000 Validation Accuracy: 0.763000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:   0.015346 Training Accuracy: 1.000000 Validation Accuracy: 0.764400\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:   0.015589 Training Accuracy: 1.000000 Validation Accuracy: 0.768000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:   0.013778 Training Accuracy: 1.000000 Validation Accuracy: 0.766200\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:   0.015854 Training Accuracy: 1.000000 Validation Accuracy: 0.760800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:   0.016833 Training Accuracy: 1.000000 Validation Accuracy: 0.762600\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:   0.016697 Training Accuracy: 1.000000 Validation Accuracy: 0.763400\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:   0.014677 Training Accuracy: 1.000000 Validation Accuracy: 0.765800\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:   0.016820 Training Accuracy: 1.000000 Validation Accuracy: 0.759600\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:   0.016965 Training Accuracy: 1.000000 Validation Accuracy: 0.765000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:   0.014130 Training Accuracy: 1.000000 Validation Accuracy: 0.762400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:   0.014266 Training Accuracy: 1.000000 Validation Accuracy: 0.765400\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:   0.015553 Training Accuracy: 0.998762 Validation Accuracy: 0.765400\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:   0.020005 Training Accuracy: 1.000000 Validation Accuracy: 0.758800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:   0.017789 Training Accuracy: 1.000000 Validation Accuracy: 0.760200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:   0.015782 Training Accuracy: 1.000000 Validation Accuracy: 0.768200\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:   0.015649 Training Accuracy: 1.000000 Validation Accuracy: 0.765400\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:   0.019704 Training Accuracy: 0.998762 Validation Accuracy: 0.763200\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:   0.020668 Training Accuracy: 0.998762 Validation Accuracy: 0.757000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:   0.017049 Training Accuracy: 1.000000 Validation Accuracy: 0.757600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:   0.014040 Training Accuracy: 1.000000 Validation Accuracy: 0.768200\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:   0.017084 Training Accuracy: 1.000000 Validation Accuracy: 0.766800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:   0.022275 Training Accuracy: 1.000000 Validation Accuracy: 0.761000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:   0.017074 Training Accuracy: 1.000000 Validation Accuracy: 0.762000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:   0.013345 Training Accuracy: 1.000000 Validation Accuracy: 0.766600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:   0.016872 Training Accuracy: 1.000000 Validation Accuracy: 0.772200\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:   0.014484 Training Accuracy: 1.000000 Validation Accuracy: 0.765200\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:   0.019423 Training Accuracy: 0.998762 Validation Accuracy: 0.757800\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:   0.019132 Training Accuracy: 1.000000 Validation Accuracy: 0.759200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:   0.012466 Training Accuracy: 1.000000 Validation Accuracy: 0.765400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:   0.015173 Training Accuracy: 1.000000 Validation Accuracy: 0.765000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:   0.015109 Training Accuracy: 1.000000 Validation Accuracy: 0.766400\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:   0.015768 Training Accuracy: 1.000000 Validation Accuracy: 0.761600\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:   0.017769 Training Accuracy: 1.000000 Validation Accuracy: 0.758200\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:   0.011721 Training Accuracy: 1.000000 Validation Accuracy: 0.760800\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:   0.011268 Training Accuracy: 1.000000 Validation Accuracy: 0.767800\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:   0.013262 Training Accuracy: 1.000000 Validation Accuracy: 0.770200\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:   0.013331 Training Accuracy: 1.000000 Validation Accuracy: 0.766800\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:   0.013897 Training Accuracy: 1.000000 Validation Accuracy: 0.759800\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:   0.010136 Training Accuracy: 1.000000 Validation Accuracy: 0.763400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:   0.009256 Training Accuracy: 1.000000 Validation Accuracy: 0.767000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:   0.013140 Training Accuracy: 1.000000 Validation Accuracy: 0.767000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:   0.015001 Training Accuracy: 1.000000 Validation Accuracy: 0.759000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:   0.013063 Training Accuracy: 1.000000 Validation Accuracy: 0.759000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:   0.010055 Training Accuracy: 1.000000 Validation Accuracy: 0.762600\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:   0.011235 Training Accuracy: 1.000000 Validation Accuracy: 0.768400\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:   0.014050 Training Accuracy: 1.000000 Validation Accuracy: 0.764200\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:   0.014790 Training Accuracy: 1.000000 Validation Accuracy: 0.762600\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:   0.014056 Training Accuracy: 1.000000 Validation Accuracy: 0.768200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:   0.010529 Training Accuracy: 1.000000 Validation Accuracy: 0.766800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7577917575836182\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAqORUREREQSBcciIiIiIsn/B8XLxrHADuo7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93b789ef60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
